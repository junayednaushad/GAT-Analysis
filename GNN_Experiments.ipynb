{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages\n",
        "May need to restart runtime after installing new packages"
      ],
      "metadata": {
        "id": "ABMYbnu9cYp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FswrHT6lUHaC"
      },
      "outputs": [],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3fqz0ioUL6K"
      },
      "outputs": [],
      "source": [
        "!pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import packages"
      ],
      "metadata": {
        "id": "gzJrC1j6ceOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IJOX4-XUa2z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksyk1Vh3DbU1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFi36z-MDex5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuration/Hyper-Parameters"
      ],
      "metadata": {
        "id": "a7ugBi72chVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRB2RUgECORF"
      },
      "outputs": [],
      "source": [
        "exp_config = {\n",
        "    'model': 'GAT',\n",
        "    'skip': None,\n",
        "    'dataset': 'Cora',\n",
        "    'hid_dim': 8,\n",
        "    'n_layers': 2,\n",
        "    'dropout_ratio': 0.6,\n",
        "    'in_heads': 8,\n",
        "    'out_heads': 1,\n",
        "    'self_loop': True,\n",
        "    'Beta': None,\n",
        "    'epochs': 1000,\n",
        "    'batch_size': None,\n",
        "    'max_patience': 100,\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.005,\n",
        "    'weight_decay':5e-4,\n",
        "}\n",
        "run_name = '{}_{}'.format(exp_config['dataset'], exp_config['model'])\n",
        "print(run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets"
      ],
      "metadata": {
        "id": "5oUY386rcpjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baL9xWcMH5ep"
      },
      "outputs": [],
      "source": [
        "if exp_config['dataset'] == 'PPI':\n",
        "    train_dataset = torch_geometric.datasets.PPI(root='./', split='train')\n",
        "    val_dataset = torch_geometric.datasets.PPI(root='./', split='val')\n",
        "    test_dataset = torch_geometric.datasets.PPI(root='./', split='test')\n",
        "\n",
        "    print('Train: ', train_dataset.data)\n",
        "    print('Val: ', val_dataset.data)\n",
        "    print('Test: ', test_dataset.data)\n",
        "\n",
        "    train_loader = torch_geometric.loader.DataLoader(train_dataset, exp_config['batch_size'], shuffle=True, pin_memory=True, num_workers=2)\n",
        "    val_loader = torch_geometric.loader.DataLoader(val_dataset, exp_config['batch_size'], shuffle=False, pin_memory=True, num_workers=2)\n",
        "    test_loader = torch_geometric.loader.DataLoader(test_dataset, exp_config['batch_size'], shuffle=False, pin_memory=True, num_workers=2)\n",
        "elif exp_config['dataset'] in ['Cornell', 'Texas', 'Wisconsin']:\n",
        "    dataset = torch_geometric.datasets.WebKB(root='./', name=exp_config['dataset'], transform=torch_geometric.transforms.NormalizeFeatures())\n",
        "    print(dataset[0])\n",
        "else:\n",
        "    dataset = torch_geometric.datasets.Planetoid(\n",
        "        root=\"./\",\n",
        "        name=exp_config['dataset'],\n",
        "        split=\"public\",\n",
        "        transform=torch_geometric.transforms.NormalizeFeatures()\n",
        "    )\n",
        "    print(dataset.data)\n",
        "    print('Training nodes:', dataset.data.train_mask.sum().item())\n",
        "    print('Validation nodes:', dataset.data.val_mask.sum().item())\n",
        "    print('Testing nodes:', dataset.data.test_mask.sum().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "eha6ECJUctGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzXA0b5BheiB"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.models.basic_gnn import GATv2Conv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hid_dim, n_classes, n_layers, dropout_ratio, skip):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.skip = skip\n",
        "\n",
        "        if self.skip:\n",
        "            assert self.n_layers >= 3\n",
        "\n",
        "        if self.n_layers == 0:\n",
        "            self.net = nn.Linear(self.input_dim, self.n_classes)\n",
        "        elif self.n_layers == 1:\n",
        "            self.net = GCNConv(self.input_dim, self.n_classes)\n",
        "        else:\n",
        "            self.net = nn.ModuleList()\n",
        "            for i in range(self.n_layers):\n",
        "                if i == 0:\n",
        "                    self.net.append(GCNConv(self.input_dim, self.hid_dim))\n",
        "                    self.net.append(nn.ReLU())\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "                elif i == self.n_layers - 1:\n",
        "                    self.net.append(GCNConv(self.hid_dim, self.n_classes))\n",
        "                else:\n",
        "                    self.net.append(GCNConv(self.hid_dim, self.hid_dim))\n",
        "                    self.net.append(nn.ReLU())\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        if self.n_layers == 0:\n",
        "            X = self.net(X)\n",
        "        elif self.n_layers == 1:\n",
        "            X = self.net(X, A)\n",
        "        else:\n",
        "            for i, layer in enumerate(self.net[:-1]):\n",
        "                if i%3 == 0: # GCNConv layer\n",
        "                    if i == 0 and self.skip:\n",
        "                        X = layer(X, A)\n",
        "                        prev = torch.clone(X)\n",
        "                    elif self.skip:\n",
        "                        X = layer(X, A)\n",
        "                        X = X + prev\n",
        "                        prev = torch.clone(X)\n",
        "                    else:\n",
        "                        X = layer(X, A)\n",
        "                else: # ReLU or Dropout layer\n",
        "                    X = layer(X)\n",
        "            \n",
        "            # final layer (classifier)\n",
        "            X = self.net[-1](X, A)\n",
        "        return X\n",
        "\n",
        "\n",
        "from torch_geometric.nn import GATConv, GATv2Conv\n",
        "\n",
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_type, input_dim, hid_dim, n_classes, n_layers, in_heads, out_heads, self_loop, dropout_ratio, skip):\n",
        "        super().__init__()\n",
        "        if layer_type == 'GAT':\n",
        "            self.gconv = getattr(torch_geometric.nn, 'GATConv')\n",
        "        if layer_type == 'GATv2':\n",
        "            self.gconv = getattr(torch_geometric.nn, 'GATv2Conv')\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.in_heads = in_heads\n",
        "        self.out_heads = out_heads\n",
        "        self.self_loop = self_loop\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.skip = skip\n",
        "\n",
        "        if self.skip:\n",
        "            assert self.n_layers >= 3\n",
        "\n",
        "        if self.n_layers == 0:\n",
        "            self.net = nn.Linear(self.input_dim, self.n_classes)\n",
        "        elif self.n_layers == 1:\n",
        "            self.net = self.gconv(self.input_dim, self.n_classes, self.out_heads, concat=False, dropout=self.dropout_ratio, add_self_loops=self.self_loop)\n",
        "        else:\n",
        "            self.net = nn.ModuleList()\n",
        "            for i in range(self.n_layers):\n",
        "                if i == 0:\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "                    self.net.append(self.gconv(self.input_dim, self.hid_dim, self.in_heads, dropout=self.dropout_ratio, add_self_loops=self.self_loop))\n",
        "                    self.net.append(nn.ELU())\n",
        "                elif i == self.n_layers - 1:\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "                    self.net.append(self.gconv(self.hid_dim*self.in_heads, self.n_classes, self.out_heads, concat=False, dropout=self.dropout_ratio, add_self_loops=self.self_loop))\n",
        "                else:\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "                    self.net.append(self.gconv(self.hid_dim*self.in_heads, self.hid_dim, self.in_heads, dropout=self.dropout_ratio, add_self_loops=self.self_loop))\n",
        "                    self.net.append(nn.ELU())\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        if self.n_layers == 0:\n",
        "            X = self.net(X)\n",
        "        elif self.n_layers == 1:\n",
        "            X = self.net(X, A)\n",
        "        else:\n",
        "            for i, layer in enumerate(self.net[:-1]):\n",
        "                if isinstance(layer, GATConv) or isinstance(layer, GATv2Conv):\n",
        "                    if i == 1 and self.skip:\n",
        "                        X = layer(X, A)\n",
        "                        prev = torch.clone(X)\n",
        "                    elif self.skip:\n",
        "                        X = layer(X, A)\n",
        "                        X = X + prev\n",
        "                        prev = torch.clone(X)\n",
        "                    else:\n",
        "                        X = layer(X, A)\n",
        "                else:\n",
        "                    X = layer(X)\n",
        "            \n",
        "            X = self.net[-1](X, A)\n",
        "        return X\n",
        "\n",
        "\n",
        "from torch_geometric.nn import AGNNConv\n",
        "\n",
        "class AGNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hid_dim, n_classes, n_layers, dropout_ratio, beta):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.beta = beta\n",
        "\n",
        "        self.net = nn.ModuleList()\n",
        "        self.net.append(nn.Linear(self.input_dim, self.hid_dim))\n",
        "        self.net.append(nn.ReLU())\n",
        "        self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "        for _ in range(self.n_layers):\n",
        "            self.net.append(AGNNConv(requires_grad=self.beta))\n",
        "        self.net.append(nn.Linear(self.hid_dim, self.n_classes))\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, AGNNConv):\n",
        "                X = layer(X, A)\n",
        "            else:\n",
        "                X = layer(X)\n",
        "        return X\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hid_dim, n_classes, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "        self.net = nn.ModuleList()\n",
        "        if self.n_layers == 1:\n",
        "            self.net.append(nn.Linear(self.input_dim, self.n_classes))\n",
        "        else:\n",
        "            for i in range(self.n_layers):\n",
        "                if i == 0:\n",
        "                    self.net.append(nn.Linear(self.input_dim, self.hid_dim))\n",
        "                    self.net.append(nn.ReLU())\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "                elif i == self.n_layers-1:\n",
        "                    self.net.append(nn.Linear(self.hid_dim, self.n_classes))\n",
        "                else:\n",
        "                    self.net.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "                    self.net.append(nn.ReLU())\n",
        "                    self.net.append(nn.Dropout(self.dropout_ratio))\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        for layer in self.net:\n",
        "            X = layer(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training, validation, and testing loops"
      ],
      "metadata": {
        "id": "uecFMrS7c6Cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl92-2CjEIVo"
      },
      "outputs": [],
      "source": [
        "def train_eval_transductive(config):\n",
        "    X = dataset[0].x.to(device)\n",
        "    A = dataset[0].edge_index.to(device)\n",
        "    y = dataset[0].y.to(device)\n",
        "    train_mask = dataset[0].train_mask.to(device)\n",
        "    val_mask = dataset[0].val_mask.to(device)\n",
        "    y_train = y[train_mask]\n",
        "    y_val = y[val_mask]\n",
        "\n",
        "    if config.model == 'GCN':\n",
        "        model = GCN(input_dim=dataset.num_features, \n",
        "                    hid_dim=config.hid_dim, \n",
        "                    n_classes=dataset.num_classes, \n",
        "                    n_layers=config.n_layers, \n",
        "                    dropout_ratio=config.dropout_ratio, \n",
        "                    skip=config.skip)\n",
        "    if config.model == 'GAT' or config.model == 'GATv2':\n",
        "        model = GAT(layer_type=config.model,\n",
        "                    input_dim=dataset.num_features, \n",
        "                    hid_dim=config.hid_dim, \n",
        "                    n_classes=dataset.num_classes, \n",
        "                    n_layers=config.n_layers, \n",
        "                    in_heads=config.in_heads, \n",
        "                    out_heads=config.out_heads, \n",
        "                    self_loop=config.self_loop, \n",
        "                    dropout_ratio=config.dropout_ratio, \n",
        "                    skip=config.skip)\n",
        "    if config.model == 'AGNN':\n",
        "        model = AGNN(input_dim=dataset.num_features,\n",
        "                     hid_dim=config.hid_dim,\n",
        "                     n_classes=dataset.num_classes,\n",
        "                     n_layers=config.n_layers,\n",
        "                     dropout_ratio=config.dropout_ratio,\n",
        "                     beta=config.Beta)\n",
        "    if config.model == 'MLP':\n",
        "        model = MLP(input_dim=dataset.num_features,\n",
        "                    hid_dim=config.hid_dim,\n",
        "                    n_classes=dataset.num_classes,\n",
        "                    n_layers=config.n_layers,\n",
        "                    dropout_ratio=config.dropout_ratio)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = getattr(torch.optim, exp_config['optimizer'])(params=model.parameters(),\n",
        "                                                              lr=config.lr,\n",
        "                                                              weight_decay=config.weight_decay) \n",
        "\n",
        "    print('Training and validating {}'.format(config.model))\n",
        "    best_val_acc = 0\n",
        "    best_val_loss = 1e10\n",
        "    patience = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X, A)\n",
        "        output_train = output[train_mask]\n",
        "\n",
        "        train_loss = criterion(output_train, y_train)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, pred_train = torch.max(output_train, 1)\n",
        "        train_acc = y_train.eq(pred_train).sum() / len(y_train)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(X, A)\n",
        "            output_val = output[val_mask]\n",
        "\n",
        "            val_loss = criterion(output_val, y_val)\n",
        "            _, pred_val = torch.max(output_val, 1)\n",
        "            val_acc = y_val.eq(pred_val).sum() / len(y_val)\n",
        "\n",
        "            # wandb.log({'train loss':train_loss, 'train accuracy':train_acc,\n",
        "            #        'val loss':val_loss, 'val accuracy':val_acc})\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                # wandb.run.summary['best val accuracy'] = best_val_acc\n",
        "                # wandb.run.summary['best epoch'] = epoch\n",
        "            \n",
        "            if val_loss < (best_val_loss * 0.99):\n",
        "                best_val_loss = val_loss\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience == config.max_patience:\n",
        "                    print('Early stopping at Epoch {}'.format(epoch))\n",
        "                    return model\n",
        "                \n",
        "    return model\n",
        "\n",
        "\n",
        "def test_transductive(model):\n",
        "    print('Testing model')\n",
        "    X = dataset[0].x.to(device)\n",
        "    A = dataset[0].edge_index.to(device)\n",
        "    y = dataset[0].y.to(device)\n",
        "    test_mask = dataset[0].test_mask.to(device)\n",
        "    y_test = y[test_mask]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X, A)\n",
        "        output_test = output[test_mask]\n",
        "        _, pred = torch.max(output_test, 1)\n",
        "        acc = y_test.eq(pred).sum() / len(y_test)\n",
        "        # wandb.run.summary['test accuracy'] = acc\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBiqMbWSVD39"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_eval_inductive(config):\n",
        "    sig = nn.Sigmoid()\n",
        "\n",
        "    if config.model == 'GCN':\n",
        "        model = GCN(input_dim=train_dataset.num_features, \n",
        "                    hid_dim=config.hid_dim, \n",
        "                    n_classes=train_dataset.num_classes, \n",
        "                    n_layers=config.n_layers, \n",
        "                    dropout_ratio=config.dropout_ratio, \n",
        "                    skip=config.skip)\n",
        "    if config.model == 'GAT' or config.model == 'GATv2':\n",
        "        model = GAT(layer_type=config.model,\n",
        "                    input_dim=train_dataset.num_features, \n",
        "                    hid_dim=config.hid_dim, \n",
        "                    n_classes=train_dataset.num_classes, \n",
        "                    n_layers=config.n_layers, \n",
        "                    in_heads=config.in_heads, \n",
        "                    out_heads=config.out_heads, \n",
        "                    self_loop=config.self_loop, \n",
        "                    dropout_ratio=config.dropout_ratio, \n",
        "                    skip=config.skip)\n",
        "    if config.model == 'AGNN':\n",
        "        model = AGNN(input_dim=train_dataset.num_features,\n",
        "                     hid_dim=config.hid_dim,\n",
        "                     n_classes=train_dataset.num_classes,\n",
        "                     n_layers=config.n_layers,\n",
        "                     dropout_ratio=config.dropout_ratio,\n",
        "                     beta=config.Beta)\n",
        "    if config.model == 'MLP':\n",
        "        model = MLP(input_dim=train_dataset.num_features,\n",
        "                    hid_dim=config.hid_dim,\n",
        "                    n_classes=train_dataset.num_classes,\n",
        "                    n_layers=config.n_layers,\n",
        "                    dropout_ratio=config.dropout_ratio)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = getattr(torch.optim, exp_config['optimizer'])(params=model.parameters(),\n",
        "                                                              lr=config.lr,\n",
        "                                                              weight_decay=config.weight_decay) \n",
        "    \n",
        "    print('Training and validating {}'.format(config.model))\n",
        "    best_val_f1 = 0\n",
        "    best_val_loss = 1e10\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        train_loss = []\n",
        "        train_f1 = []\n",
        "\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            x = data.x\n",
        "            A = data.edge_index\n",
        "            y = data.y\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x, A)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            preds = torch.round(sig(output))\n",
        "            preds = preds.detach().to('cpu').numpy()\n",
        "            train_f1.append(f1_score(y.to('cpu').numpy(), preds, average='micro'))\n",
        "        \n",
        "        model.eval()\n",
        "        val_loss = []\n",
        "        val_f1 = []\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                x = data.x\n",
        "                A = data.edge_index\n",
        "                y = data.y\n",
        "\n",
        "                output = model(x, A)\n",
        "                loss = criterion(output, y)\n",
        "                val_loss.append(loss.item())\n",
        "\n",
        "                preds = torch.round(sig(output))\n",
        "                preds = preds.to('cpu').numpy()\n",
        "                val_f1.append(f1_score(y.to('cpu').numpy(), preds, average='micro'))\n",
        "        \n",
        "\n",
        "        train_loss = torch.tensor(train_loss).mean()\n",
        "        train_f1 = torch.tensor(train_f1).mean()\n",
        "        val_loss = torch.tensor(val_loss).mean()\n",
        "        val_f1 = torch.tensor(val_f1).mean()\n",
        "\n",
        "        wandb.log({'train loss':train_loss, 'train f1':train_f1,\n",
        "                   'val loss':val_loss, 'val f1':val_f1})\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            wandb.run.summary['best val f1'] = best_val_f1\n",
        "            wandb.run.summary['best epoch'] = epoch\n",
        "        \n",
        "        if val_loss < (best_val_loss * 0.99):\n",
        "            best_val_loss = val_loss\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience == config.max_patience:\n",
        "                print('Early stopping at Epoch {}'.format(epoch))\n",
        "                return model\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_inductive(model):\n",
        "    print('Testing model')\n",
        "    sig = nn.Sigmoid()\n",
        "\n",
        "    test_f1 = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader):\n",
        "            data = data.to(device)\n",
        "            x = data.x\n",
        "            A = data.edge_index\n",
        "            y = data.y\n",
        "\n",
        "            output = model(x, A)\n",
        "            preds = torch.round(sig(output))\n",
        "            preds = preds.to('cpu').numpy()\n",
        "            test_f1.append(f1_score(y.to('cpu').numpy(), preds, average='micro'))\n",
        "        \n",
        "    # wandb.run.summary['test f1'] = torch.tensor(test_f1).mean().item()\n",
        "    return torch.tensor(test_f1).mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_eval_hetero(config):\n",
        "    X = dataset[0].x.to(device)\n",
        "    A = dataset[0].edge_index.to(device)\n",
        "    y = dataset[0].y.to(device)\n",
        "\n",
        "    test_results = []\n",
        "    print('Training and validating {}'.format(config.model))\n",
        "    for i in tqdm(range(10)):\n",
        "        train_mask, test_mask = train_test_split(np.arange(len(dataset[0].x)), test_size=0.2, random_state=i)\n",
        "        y_train = y[train_mask]\n",
        "        y_test = y[test_mask]\n",
        "\n",
        "        if config.model == 'GCN':\n",
        "            model = GCN(input_dim=dataset.num_features, \n",
        "                        hid_dim=config.hid_dim, \n",
        "                        n_classes=dataset.num_classes, \n",
        "                        n_layers=config.n_layers, \n",
        "                        dropout_ratio=config.dropout_ratio, \n",
        "                        skip=config.skip)\n",
        "        if config.model == 'GAT' or config.model == 'GATv2':\n",
        "            model = GAT(layer_type=config.model,\n",
        "                        input_dim=dataset.num_features, \n",
        "                        hid_dim=config.hid_dim, \n",
        "                        n_classes=dataset.num_classes, \n",
        "                        n_layers=config.n_layers, \n",
        "                        in_heads=config.in_heads, \n",
        "                        out_heads=config.out_heads, \n",
        "                        self_loop=config.self_loop, \n",
        "                        dropout_ratio=config.dropout_ratio, \n",
        "                        skip=config.skip)\n",
        "        if config.model == 'AGNN':\n",
        "            model = AGNN(input_dim=dataset.num_features,\n",
        "                        hid_dim=config.hid_dim,\n",
        "                        n_classes=dataset.num_classes,\n",
        "                        n_layers=config.n_layers,\n",
        "                        dropout_ratio=config.dropout_ratio,\n",
        "                        beta=config.Beta)\n",
        "        if config.model == 'MLP':\n",
        "            model = MLP(input_dim=dataset.num_features,\n",
        "                        hid_dim=config.hid_dim,\n",
        "                        n_classes=dataset.num_classes,\n",
        "                        n_layers=config.n_layers,\n",
        "                        dropout_ratio=config.dropout_ratio)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = getattr(torch.optim, exp_config['optimizer'])(params=model.parameters(),\n",
        "                                                                lr=config.lr,\n",
        "                                                                weight_decay=config.weight_decay) \n",
        "\n",
        "    \n",
        "        for epoch in range(config.epochs):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(X, A)\n",
        "            output_train = output[train_mask]\n",
        "\n",
        "            train_loss = criterion(output_train, y_train)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, pred_train = torch.max(output_train, 1)\n",
        "            train_acc = (y_train.eq(pred_train).sum() / len(y_train)).item()\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print('{}.\\tEPOCH:{}\\tTRAIN ACC:{:.4}'.format(i, epoch, train_acc))\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(X, A)\n",
        "            output_test = output[test_mask]\n",
        "            _, pred_test = torch.max(output_test, 1)\n",
        "            test_acc = (y_test.eq(pred_test).sum() / len(y_test)).item()\n",
        "            test_results.append(test_acc)\n",
        "            print('TEST ACC:{:.4}'.format(test_acc))\n",
        "        print()\n",
        "    return test_results"
      ],
      "metadata": {
        "id": "urvvBWh-ZTYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run experiment"
      ],
      "metadata": {
        "id": "Qt2Oz4Zhe28r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJaZqq1iG0Fz"
      },
      "outputs": [],
      "source": [
        "n_runs = 10\n",
        "with wandb.init(project='GRL', name=run_name, config=exp_config, tags=run_name.split('_')):\n",
        "    config = wandb.config\n",
        "    test_results = []\n",
        "    if config.dataset == 'PPI':\n",
        "        for _ in range(n_runs):\n",
        "            best_model = train_eval_inductive(config)\n",
        "            test_results.append(test_inductive(best_model))\n",
        "    elif config.dataset in ['Cornell', 'Texas', 'Wisconsin']:\n",
        "        test_results = train_eval_hetero(config)\n",
        "    else:\n",
        "        for _ in range(n_runs):\n",
        "            best_model = train_eval_transductive(config)\n",
        "            test_results.append(test_transductive(best_model))\n",
        "\n",
        "    test_results = torch.tensor(test_results)\n",
        "    wandb.run.summary['mean test'] = test_results.mean().item()\n",
        "    wandb.run.summary['std test'] = test_results.std().item()\n",
        "\n",
        "# Code for running experiments with different depths:\n",
        "# with wandb.init(project='GRL', name=run_name, config=exp_config, tags=run_name.split('_')):\n",
        "#     config = wandb.config\n",
        "#     layers = np.arange(1,11)\n",
        "#     n_runs = 5\n",
        "#     results_by_depth = []\n",
        "#     for n_layers in layers:\n",
        "#         config.n_layers = n_layers\n",
        "#         if n_layers >= 3:\n",
        "#             config.skip = True\n",
        "#         else:\n",
        "#             config.skip = False\n",
        "\n",
        "#         test_results = []\n",
        "#         if config.dataset == 'PPI':\n",
        "#             for _ in range(n_runs):\n",
        "#                 best_model = train_eval_inductive(config)\n",
        "#                 test_results.append(test_inductive(best_model))\n",
        "#         else:\n",
        "#             for _ in range(n_runs):\n",
        "#                 best_model = train_eval_transductive(config)\n",
        "#                 test_results.append(test_transductive(best_model))\n",
        "\n",
        "#         test_results = torch.tensor(test_results)\n",
        "#         results_by_depth.append(test_results.mean().item())\n",
        "#     wandb.run.summary['test_results_by_depth'] = results_by_depth"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}